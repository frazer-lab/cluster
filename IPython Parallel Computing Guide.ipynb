{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IPython Parallel Computing Guide\n",
    "\n",
    "This guide is intended for IPython 4.0 or greater and Jupyter notebooks. \n",
    "Make sure you install `ipyparallel` using `conda install ipyparallel`.\n",
    "Throughout this guide I assume you are wanting to connect to an IPython cluster\n",
    "via a Jupyter notebook. I think it's possible to connect via a command line\n",
    "IPython interactive session as well.\n",
    "\n",
    "This guide will help you get an IPython cluster up and running on our cluster.\n",
    "To learn the different ways of running code etc. on an IPython cluster, you \n",
    "should consult the [`ipyparallel` documentation](https://ipyparallel.readthedocs.org/en/latest/index.html). \n",
    "[This page](https://ipyparallel.readthedocs.org/en/latest/multiengine.html) is a \n",
    "good starting place after you've gone through this notebook.\n",
    "\n",
    "## Local Parallel Computing\n",
    "\n",
    "This first part is based on [this page](https://ipyparallel.readthedocs.org/en/latest/intro.html#getting-started).\n",
    "\n",
    "The simplest way to do execute code in parallel is by starting an IPython cluster on \n",
    "the local machine. For instance, if you are logged in on a head node, you can start an \n",
    "IPython cluster with four engines (cores) by executing \n",
    "\n",
    "    ipcluster start -n 4\n",
    "    \n",
    "at the command line. You should run this in a screen because you will need this\n",
    "command to continue running as long as you want to use the IPython cluster. You \n",
    "could also run this command in the background (writing the logs to a file I guess).\n",
    "\n",
    "Note that you probably want to have the same `conda` environment loaded when you launch the \n",
    "cluster that you are using to run this notebook. Using different environments may not\n",
    "cause a problem although it would be confusing because the engines' Python environment\n",
    "would be different than the environment of this notebook.\n",
    "    \n",
    "Now that we've started the cluster, we can import `ipyparallel` in this notebook and create a client. The\n",
    "client automatically looks for the cluster and connects to it. If you are using\n",
    "an IPython profile besides the default, you may need `c = Client(profile='myprofile')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ipyparallel import Client\n",
    "# Connect to client.\n",
    "c = Client()\n",
    "# You may need to specify the profile: c = Client(profile='myprofile')\n",
    "\n",
    "# Show the IDs of the cluster engines:\n",
    "c.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello, World', 'Hello, World', 'Hello, World', 'Hello, World']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run some code on each cluster engine.\n",
    "c[:].apply_sync(lambda : \"Hello, World\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello, World prints four times because the code was executed on each one\n",
    "of our four engines.\n",
    "\n",
    "### Stopping the cluster\n",
    "\n",
    "You should be able to use\n",
    "\n",
    "    ipcluster stop\n",
    "    \n",
    "or \n",
    "\n",
    "    ipcluster stop --profile myprofile\n",
    "    \n",
    "at the command line to stop the IPython cluster. You can also kill the process\n",
    "wherever it's running, via top, etc.\n",
    "\n",
    "## SGE Parallel Computing\n",
    "\n",
    "Now we'll look at controlling the IPython cluster from the head node but executing code in parallel \n",
    "on the scheduled nodes **and/or** on the head node as well. \n",
    "This second part is based on [this page](https://ipyparallel.readthedocs.org/en/latest/process.html)\n",
    "although the page contains a lot of stuff not relevant to our cluster set up.\n",
    "\n",
    "When running an IPython cluster on a local machine as we did above, you can start the \n",
    "cluster with `ipcluster` which launches both a controller and engines. The controller\n",
    "coordinates the task of sending code out to the engines while the engines actually do the\n",
    "work. Here, we want to run a controller on the head node and engines out on the scheduled \n",
    "nodes (and potentially on the head node as well). This will allow us to farm out parallel\n",
    "computations from notebooks running on the headnode.\n",
    "\n",
    "An alternative strategy could be to\n",
    "configure the IPython cluster to use SGE and use `ipcluster` to launch an entire cluster\n",
    "(controller and engines) on a scheduled node. In this scenario, you would need to SSH \n",
    "to that node and launch a notebook to connect to the cluster. This is a bit harder since\n",
    "we don't have web servers running on the scheduled nodes, so I guess you would need to SSH \n",
    "tunnel to those notebook to see them. If you look online you will probably see people \n",
    "using this strategy because this is what you would do on a cluster/cloud system like SDSC\n",
    "or Amazon.\n",
    "\n",
    "### Set up\n",
    "\n",
    "This set up only needs to be done once to get an IPython profile with some parameters\n",
    "particular to our cluster. First we'll create an IPython profile with parallel config files:\n",
    "\n",
    "    ipython profile create parallel --parallel\n",
    "\n",
    "Change to `~/.ipython/profile_parallel`. We want our controller to listen for \n",
    "external connections. Because we are on the head node, we are on a trusted network, \n",
    "so we can listen for all external connections. Open `ipcontroller_config.py` and \n",
    "uncomment/change the following line to allow all external connections:\n",
    "\n",
    "    c.HubFactory.engine_ip = '*'\n",
    "    \n",
    "Also add the line\n",
    "\n",
    "    c.HubFactory.ip = '*'\n",
    "    \n",
    "right below this.\n",
    "\n",
    "### Conda environment\n",
    "\n",
    "Note that you probably want to use the same `conda` environment that this notebook is running\n",
    "under for all of the steps below. \n",
    "Using different environments may not cause a problem although it would definitely be confusing.\n",
    "\n",
    "### Launch controller\n",
    "\n",
    "You can launch the controller on the head node using \n",
    "\n",
    "    ipcontroller --profile=parallel\n",
    "    \n",
    "while logged into the head node. Run this in a screen so it can in the background (or run\n",
    "in the background and write the logs to files). This controller\n",
    "isn't doing anything except waiting for engines so that it can manage them. You should\n",
    "see some output like\n",
    "\n",
    "    2015-12-02 16:29:16.970 [IPControllerApp] Hub listening on tcp://*:48181 for registration.\n",
    "    2015-12-02 16:29:16.971 [IPControllerApp] Hub using DB backend: 'NoDB'\n",
    "    2015-12-02 16:29:17.244 [IPControllerApp] hub::created hub\n",
    "    2015-12-02 16:29:17.244 [IPControllerApp] writing connection info to /frazer01/home/cdeboever/.ipython/profile_parallel/security/ipcontroller-client.json\n",
    "    2015-12-02 16:29:17.248 [IPControllerApp] writing connection info to /frazer01/home/cdeboever/.ipython/profile_parallel/security/ipcontroller-engine.json\n",
    "    2015-12-02 16:29:17.250 [IPControllerApp] task::using Python leastload Task scheduler\n",
    "    2015-12-02 16:29:17.251 [IPControllerApp] Heartmonitor started\n",
    "    2015-12-02 16:29:17.263 [IPControllerApp] Creating pid file: /frazer01/home/cdeboever/.ipython/profile_parallel/pid/ipcontroller.pid\n",
    "    2015-12-02 16:29:17.273 [scheduler] Scheduler started [leastload]\n",
    "    2015-12-02 16:29:17.275 [IPControllerApp] client::client '\\x00o\\xee1b' requested u'connection_request'\n",
    "    2015-12-02 16:29:17.275 [IPControllerApp] client::client ['\\x00o\\xee1b'] connected\n",
    "\n",
    "### Launch engines on the head node\n",
    "\n",
    "First, let's try launching some engines on the head node. You can launch an engine\n",
    "on the head node using\n",
    "\n",
    "    ipengine --profile=parallel\n",
    "    \n",
    "You should see some output like\n",
    "\n",
    "    2015-12-02 16:30:03.773 [IPEngineApp] Loading url_file u'/frazer01/home/cdeboever/.ipython/profile_parallel/security/ipcontroller-engine.json'\n",
    "    2015-12-02 16:30:03.904 [IPEngineApp] Registering with controller at tcp://127.0.0.1:48181\n",
    "    2015-12-02 16:30:03.974 [IPEngineApp] Starting to monitor the heartbeat signal from the hub every 3010 ms.\n",
    "    2015-12-02 16:30:03.978 [IPEngineApp] Completed registration with id 0\n",
    "    \n",
    "The `ipcontroller-engine.json` file was created when we launched our controller and tells\n",
    "the engines where to find the controller so they can connect. If an engine can't find a controller,\n",
    "it will die. You can see from this output that our engine connected to the controller correctly.\n",
    "Now we can start a client in this notebook that will connect to our controller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipyparallel import Client\n",
    "# Connect to client.\n",
    "c = Client(profile='parallel')\n",
    "\n",
    "# Show the IDs of the cluster engines:\n",
    "c.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that right now we just have a single engine. Go ahead and start \n",
    "another engine on the head node using the same command as before:\n",
    "\n",
    "    ipengine --profile=parallel\n",
    "    \n",
    "We'll make a new client and now we can see that both engines are connected to the \n",
    "controller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Client(profile='parallel')\n",
    "\n",
    "# Show the IDs of the cluster engines:\n",
    "c.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have engines with IDs 0 and 1. If you look at the output when you \n",
    "started the last engine, you'll see something like\n",
    "\n",
    "    2015-12-02 16:30:53.280 [IPEngineApp] Loading url_file u'/frazer01/home/cdeboever/.ipython/profile_parallel/security/ipcontroller-engine.json'\n",
    "    2015-12-02 16:30:53.301 [IPEngineApp] Registering with controller at tcp://127.0.0.1:48181\n",
    "    2015-12-02 16:30:53.371 [IPEngineApp] Starting to monitor the heartbeat signal from the hub every 3010 ms.\n",
    "    2015-12-02 16:30:53.375 [IPEngineApp] Completed registration with id 1\n",
    "    \n",
    "Notice that the last line says that we just registered a new engine with id 1 which corresponds\n",
    "to what we see from `c.ids`.\n",
    "\n",
    "As before, we can run some code in parallel on both engines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello, World', 'Hello, World']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run some code on each cluster engine.\n",
    "c[:].apply_sync(lambda : \"Hello, World\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch engines on the scheduled nodes\n",
    "\n",
    "Now let's launch some engines on the scheduled nodes. \n",
    "You need to make an SGE script to launch engines on the scheduled nodes. Try making a script\n",
    "`~/test.sh` with the following contents:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#!/bin/bash\n",
    "\n",
    "#$ -N engine_test\n",
    "#$ -l h_vmem=1.0G\n",
    "#$ -pe smp 1\n",
    "#$ -S /bin/bash\n",
    "#$ -o ~/test.out\n",
    "#$ -e ~/test.err\n",
    "\n",
    "# source activate mycondaenv\n",
    "ipengine --profile=parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you may want/need to activate a particular `conda` environment in this script\n",
    "if you are using an environment besides the default for this notebook/the controller.\n",
    "You can submit the SGE scrip the schedule using `qsub ~/test.sh`. After a few moments, \n",
    "check `~/test.err`. You should see some output like:\n",
    "\n",
    "    2015-12-02 16:32:56.771 [IPEngineApp] Loading url_file u'/frazer01/home/cdeboever/.ipython/profile_parallel/security/ipcontroller-engine.json'\n",
    "    2015-12-02 16:32:57.199 [IPEngineApp] Registering with controller at tcp://169.228.63.175:48181\n",
    "    2015-12-02 16:33:07.101 [IPEngineApp] Starting to monitor the heartbeat signal from the hub every 3010 ms.\n",
    "    2015-12-02 16:33:07.212 [IPEngineApp] Completed registration with id 2\n",
    "    \n",
    "It looks like our engine connected successfully. You can see the job using `qstat':"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "job-ID  prior   name       user         state submit/start at     queue                          slots ja-task-ID \n",
    "-----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "   7137 0.45617 engine_tes cdeboever    r     12/02/2015 16:32:34 all.q@fl-n-1-10                    1   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try creating a new client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipyparallel import Client\n",
    "# Connect to client.\n",
    "c = Client(profile='parallel')\n",
    "\n",
    "# Show the IDs of the cluster engines:\n",
    "c.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have three engines available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello, World', 'Hello, World', 'Hello, World']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run some code on each cluster engine.\n",
    "c[:].apply_sync(lambda : \"Hello, World\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can run code on all three. If we kill the scheduled job (using `qdel 7137`\n",
    "in this case) that is running one of our engines, we'll get a problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "CompositeError",
     "evalue": "one or more exceptions from call to method: <lambda>\n[Engine Exception]EngineError: Engine 2 died while running task '893c9fad-266d-4659-b7b8-62b625aaf94d'",
     "output_type": "error",
     "traceback": [
      "[Engine Exception]",
      "Traceback (most recent call last):",
      "  File \"/frazer01/home/cdeboever/software/anaconda/envs/cie/lib/python2.7/site-packages/ipyparallel/client/client.py\", line 701, in _handle_stranded_msgs",
      "    raise error.EngineError(\"Engine %r died while running task %r\"%(eid, msg_id))",
      "EngineError: Engine 2 died while running task '893c9fad-266d-4659-b7b8-62b625aaf94d'",
      ""
     ]
    }
   ],
   "source": [
    "c[:].apply_sync(lambda : \"Hello, World\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the logs from the controller, we'll see something like\n",
    "\n",
    "    2015-12-02 15:42:34.660 [IPControllerApp] heartbeat::missed 6dbc2571-3838-47dc-95f8-8f56c1a22bab : 3\n",
    "    2015-12-02 15:42:37.662 [IPControllerApp] heartbeat::missed 6dbc2571-3838-47dc-95f8-8f56c1a22bab : 4\n",
    "    2015-12-02 15:42:40.663 [IPControllerApp] heartbeat::missed 6dbc2571-3838-47dc-95f8-8f56c1a22bab : 5\n",
    "    2015-12-02 15:42:43.663 [IPControllerApp] heartbeat::missed 6dbc2571-3838-47dc-95f8-8f56c1a22bab : 6\n",
    "    2015-12-02 15:42:46.663 [IPControllerApp] heartbeat::missed 6dbc2571-3838-47dc-95f8-8f56c1a22bab : 7\n",
    "    2015-12-02 15:42:49.662 [IPControllerApp] heartbeat::missed 6dbc2571-3838-47dc-95f8-8f56c1a22bab : 8\n",
    "    2015-12-02 15:42:52.663 [IPControllerApp] heartbeat::missed 6dbc2571-3838-47dc-95f8-8f56c1a22bab : 9\n",
    "    2015-12-02 15:42:55.663 [IPControllerApp] heartbeat::missed 6dbc2571-3838-47dc-95f8-8f56c1a22bab : 10\n",
    "    2015-12-02 15:42:58.663 [IPControllerApp] heartbeat::missed 6dbc2571-3838-47dc-95f8-8f56c1a22bab : 11\n",
    "    2015-12-02 15:42:58.663 [IPControllerApp] registration::unregister_engine(2)\n",
    "    \n",
    "I think the `heartbeat::missed` lines are waiting for a reply from engine 2. Eventually, the\n",
    "controller decides it won't hear back from engine 2 so it throws the above error and removes\n",
    "engine 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe that if you kill an engine, the controller will eventually remove it after \n",
    "it doesn't hear from the engine for a set amount of heartbeats.\n",
    "\n",
    "Note that if we submit a job to run an engine, it will run until it is killed by \n",
    "queue time limits. If you wanted to limit the time, you could do something like"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#!/bin/bash\n",
    "\n",
    "#$ -N engine_test\n",
    "#$ -l h_vmem=1.0G\n",
    "#$ -pe smp 1\n",
    "#$ -S /bin/bash\n",
    "#$ -o ~/test.out\n",
    "#$ -e ~/test.err\n",
    "\n",
    "# source activate mycondaenv\n",
    "ipengine --profile=parallel & \n",
    "sleep 1h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This job would finish after an hour and kill the engine.\n",
    "\n",
    "### Launch multiple engines on the scheduled nodes\n",
    "\n",
    "Now let's try to launch multiple engines on the scheduled nodes. A simple way to do this\n",
    "is to create a file `~/test2.sh` with the following contents:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#!/bin/bash\n",
    "\n",
    "#$ -N engine_test2\n",
    "#$ -l h_vmem=1.0G\n",
    "#$ -pe smp 2\n",
    "#$ -S /bin/bash\n",
    "#$ -o ~/test2.out\n",
    "#$ -e ~/test2.err\n",
    "\n",
    "# source activate mycondaenv\n",
    "ipengine --profile=parallel &\n",
    "ipengine --profile=parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit this to the scheduler with `qsub` and after a bit of time you should see \n",
    "output in `~/test2.err` like\n",
    "\n",
    "    2015-12-02 16:52:22.952 [IPEngineApp] Loading url_file u'/frazer01/home/cdeboever/.ipython/profile_parallel/security/ipcontroller-engine.json\n",
    "    '\n",
    "    2015-12-02 16:52:22.952 [IPEngineApp] Loading url_file u'/frazer01/home/cdeboever/.ipython/profile_parallel/security/ipcontroller-engine.json\n",
    "    '\n",
    "    2015-12-02 16:52:22.984 [IPEngineApp] Registering with controller at tcp://169.228.63.175:48181\n",
    "    2015-12-02 16:52:22.984 [IPEngineApp] Registering with controller at tcp://169.228.63.175:48181\n",
    "    2015-12-02 16:52:23.100 [IPEngineApp] Starting to monitor the heartbeat signal from the hub every 3010 ms.\n",
    "    2015-12-02 16:52:23.106 [IPEngineApp] Starting to monitor the heartbeat signal from the hub every 3010 ms.\n",
    "    2015-12-02 16:52:23.107 [IPEngineApp] Completed registration with id 6\n",
    "    2015-12-02 16:52:23.114 [IPEngineApp] Completed registration with id 5\n",
    "    \n",
    "We can create a new client and see these engines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 5, 6]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipyparallel import Client\n",
    "# Connect to client.\n",
    "c = Client(profile='parallel')\n",
    "\n",
    "# Show the IDs of the cluster engines:\n",
    "c.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that my engines got IDs 5 and 6 because I've been experimenting with creating\n",
    "engines and deleting them. It seems that IPython won't recycle engine numbers. Your\n",
    "numbers may be different. Once again we can kill these engines by killing the job\n",
    "using `qdel` or we can run the engines in the background and use `sleep` to kill the \n",
    "engines after a certain amount of time (see example above).\n",
    "\n",
    "#### Array jobs\n",
    "\n",
    "A more efficient way to launch multiple engines is to use \n",
    "[array jobs](https://wiki.duke.edu/display/SCSC/SGE+Array+Jobs). \n",
    "Create a file `~/test_array.sh` with"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#!/bin/bash\n",
    "\n",
    "#$ -N engine_test_array\n",
    "#$ -l h_vmem=1.0G\n",
    "#$ -pe smp 1\n",
    "#$ -S /bin/bash\n",
    "#$ -o ~/test_array.out\n",
    "#$ -e ~/test_array.err\n",
    "#$ -t 1-4\n",
    "\n",
    "source activate cie\n",
    "ipengine --profile=parallel &\n",
    "sleep 5m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script will launch four engines that will die after 5 minutes. They \n",
    "will look something like \n",
    "\n",
    "       7141 0.45617 engine_tes cdeboever    r     12/02/2015 17:07:49 all.q@fl-n-1-10                    1 1\n",
    "       7141 0.45617 engine_tes cdeboever    r     12/02/2015 17:07:49 all.q@fl-n-1-10                    1 2\n",
    "       7141 0.45617 engine_tes cdeboever    r     12/02/2015 17:07:49 all.q@fl-n-1-10                    1 3\n",
    "       7141 0.45617 engine_tes cdeboever    r     12/02/2015 17:07:49 all.q@fl-n-1-10                    1 4\n",
    "   \n",
    "in `qstat`. We can create a new client and see these engines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 5, 6]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipyparallel import Client\n",
    "# Connect to client.\n",
    "c = Client(profile='parallel')\n",
    "\n",
    "# Show the IDs of the cluster engines:\n",
    "c.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from the output above that the engines all have the same job ID,\n",
    "so you can easily kill them all with one `qdel`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping the cluster\n",
    "\n",
    "I'm not exactly sure what the best way to stop the cluster is, but killing the \n",
    "controller process (`ctrl-c` wherever it's running, use `top`, etc.) will stop the controller.\n",
    "The engines will stop after failing to hear from the controller 50 times (engines\n",
    "check for the controller ~3 seconds). You'll see something like\n",
    "\n",
    "    2015-12-02 16:11:21.074 [IPEngineApp] WARNING | No heartbeat in the last 3010 ms (50 time(s) in a row).\n",
    "    2015-12-02 16:11:21.075 [IPEngineApp] CRITICAL | Maximum number of heartbeats misses reached (50 times 3010 ms), shutting down.\n",
    "    \n",
    "from the engine logs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
